{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4QpJw7DXtsV"
      },
      "outputs": [],
      "source": [
        "!pip install kneed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import shapiro , kstest , anderson , normaltest\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from kneed import KneeLocator\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import KFold, cross_val_score , GridSearchCV ,train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score ,classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "df=pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')"
      ],
      "metadata": {
        "id": "cbJuIoWWYYEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGLFQO6yuup7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.info()"
      ],
      "metadata": {
        "id": "5LrlAYXIbckp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.drop('customerID',axis=1,inplace=True)\n",
        "df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
        "df.isnull().sum()\n",
        "df = df.fillna(df[\"TotalCharges\"].mean())\n",
        "duplicates = df[df.duplicated()]\n",
        "print(duplicates)\n",
        "\n"
      ],
      "metadata": {
        "id": "sRIXZ1aQttCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df['CorrectTotalCharges'] = df['tenure'] * df['MonthlyCharges']\n",
        "\n",
        "# Identify rows where TotalCharges is incorrect\n",
        "#incorrect_total_charges = df[df['TotalCharges'] != df['CorrectTotalCharges']]\n",
        "#incorrect_total_charges"
      ],
      "metadata": {
        "id": "g0_cQQPLJGkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"data shape : \",df.shape)\n",
        "print(\"Missing value :\\n \",df.isnull().sum())\n",
        "print(f\"Total number of samples are {df.shape[0]}\")\n",
        "print(f\"Total number of features are {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "rQZ3RFToju3E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=['int', 'float']).round(2)"
      ],
      "metadata": {
        "id": "Yiz3VVwOtnr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=['object'])"
      ],
      "metadata": {
        "id": "GUmDUUmXt5tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "WuJHSXJcxAAC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = df.fillna(df[\"TotalCharges\"].mean())"
      ],
      "metadata": {
        "id": "JP55lswZxDxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['SeniorCitizen'].replace({0: 'No', 1: 'Yes'}, inplace=True)\n",
        "df['SeniorCitizen'].value_counts()"
      ],
      "metadata": {
        "id": "D1sa7IWMuILT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df['customerID'].duplicated().sum()"
      ],
      "metadata": {
        "id": "VDzgFRIzxi4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Internet service\"] = df[\"OnlineSecurity\"] != \"No internet service\"\n",
        "df_encoded = pd.get_dummies(df, columns=['MultipleLines'])\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['InternetService'])\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['OnlineSecurity'])\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['PaymentMethod'])\n",
        "#df_encoded = pd.get_dummies(df_encoded, columns=['Contract'])\n",
        "print(df_encoded)"
      ],
      "metadata": {
        "id": "NFfuUkjMmMlr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "label_encoder = LabelEncoder()\n",
        "df['Contract_encoded'] = label_encoder.fit_transform(df['Contract'])\n",
        "print(df)\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "37MiXzjsnyPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_encoded = df_encoded.sparse.to_dense()\n",
        "df_encoded.replace({'Yes': 1, 'No': 0 , True:1 , False:0 , \"Male\":1 , \"Female\":0 , \"No internet service\":0 , \"Month-to-month\":1 , \"One year\":12 , \"Two year\":24}, inplace=True)\n",
        "print(df_encoded)"
      ],
      "metadata": {
        "id": "7iSeT1GvUofi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.value_counts())"
      ],
      "metadata": {
        "id": "GjO678OoaX--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkD7I7RNDBIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df_encoded.columns:\n",
        "    if df_encoded[column].dtype == 'object':\n",
        "        df_encoded[column] = pd.to_numeric(df_encoded[column], errors='coerce')\n",
        "\n",
        "df_encoded[\"Internet service\"] = pd.to_numeric(df_encoded[\"Internet service\"], errors='coerce')\n",
        "print(df_encoded.dtypes)"
      ],
      "metadata": {
        "id": "P5w_gRGSDRcn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_visualize = [\"MonthlyCharges\" , \"TotalCharges\" , \"tenure\"]\n",
        "\n",
        "for column in column_to_visualize:\n",
        "    plt.hist(df_encoded[column] , bins = 60 , edgecolor = \"black\")\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Distribution of {column}\")\n",
        "    plt.show()\n",
        "    plt.boxplot(df_encoded[column])\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Distribution of {column}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bQVeXEuxG5sD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in column_to_visualize:\n",
        "  stat, p = shapiro(df_encoded[column])\n",
        "  print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "  if p > 0.05:\n",
        "      print(f'according to shapiro test the data looks normally distributed for {column}')\n",
        "  else:\n",
        "      print(f'ccording to shapiro test the data does not look normally distributed for {column}')\n"
      ],
      "metadata": {
        "id": "DueAt9GMM_Lv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in column_to_visualize:\n",
        "  stat, p = kstest(df_encoded[column] , \"norm\")\n",
        "  if p > 0.05:\n",
        "      print(f'according to kstest test the data looks normally distributed for {column}')\n",
        "  else:\n",
        "      print(f'ccording to kstest test the data does not look normally distributed for {column}')"
      ],
      "metadata": {
        "id": "MKEIFcT0OeEF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df_encoded['MonthlyCharges'] , df_encoded['TotalCharges'] , label = \"Monthly and total charges\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.scatter(df_encoded['MonthlyCharges'] , df_encoded['tenure'] , label = \"Monthly and tenure\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.scatter(df_encoded['TotalCharges'] , df_encoded['tenure'] , label = \"Total charges and tenure\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MCSLKsrUUiCu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_encoded[column_to_visualize])\n",
        "df_scaled = pd.DataFrame(df_scaled)\n"
      ],
      "metadata": {
        "id": "eJ-FsVOeWnh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "k = 5\n",
        "neighbors = NearestNeighbors(n_neighbors = k)\n",
        "neighbors_fit = neighbors.fit(df_scaled)\n",
        "distances, indices = neighbors_fit.kneighbors(df_scaled)\n",
        "\n",
        "distances = np.sort(distances[:, k - 1])\n",
        "kneedle = KneeLocator(np.arange(len(distances)) , distances, curve='convex', direction='increasing')\n",
        "\n",
        "plt.plot(distances)\n",
        "plt.title(\"K-Distance Graph\")\n",
        "plt.xlabel(\"Data Points Sorted by Distance\")\n",
        "plt.ylabel(\"Distance to 7th Nearest Neighbor\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zQ6C2XJtajXH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The elbow is located at sample: {kneedle.elbow}\")"
      ],
      "metadata": {
        "id": "qcLqZbOoEnn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DBSCAN(eps=0.0375, min_samples=10).fit(df_scaled)\n",
        "labels = db.labels_\n",
        "outliers = df_scaled[labels == -1]\n",
        "print(\"Outliers detected by DBSCAN:\")\n",
        "print(len(outliers))"
      ],
      "metadata": {
        "id": "7hMp52XMOeJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled = pd.DataFrame(df_scaled)\n",
        "for column in column_to_visualize:\n",
        "    Q1 = df_encoded[column].quantile(0.25)\n",
        "    Q3 = df_encoded[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df_encoded[column] < lower_bound) | (df_encoded[column] > upper_bound)]\n",
        "    print(f\"Outliers in {column}:\")\n",
        "    print(outliers)"
      ],
      "metadata": {
        "id": "RC0HWJ5HZJl0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "isolation_forest = IsolationForest(contamination=0.02)\n",
        "isolation_forest.fit(df_scaled)\n",
        "outliers = df_scaled[isolation_forest.predict(df_scaled) == -1]"
      ],
      "metadata": {
        "id": "EaQFBGIFdpqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers.shape"
      ],
      "metadata": {
        "id": "6WdSEYxw_rS0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.head()\n",
        "df_encoded.isna().isna().sum()\n"
      ],
      "metadata": {
        "id": "LLLK4dduyCdj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_encoded.corr()['Churn'].sort_values(ascending=False)\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "WxROdEaBweik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = df_encoded.drop(columns=['Churn'])\n",
        "y = df_encoded['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Check feature importances\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "id": "B6meQHchwo42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Engineered_df = df_encoded.copy(deep = True)\n",
        "Engineered_df['num_services'] = (df_encoded[['PhoneService', 'MultipleLines_Yes', 'OnlineSecurity_Yes', 'OnlineBackup', 'DeviceProtection',\n",
        "                                          'TechSupport', 'StreamingTV', 'StreamingMovies']] == 1 ).sum(axis=1)\n",
        "\n",
        "Engineered_df['charges_ratio'] = (pd.to_numeric(df_encoded['TotalCharges'], errors='coerce') / df_encoded['MonthlyCharges'])\n",
        "Engineered_df['charges_ratio'] = (Engineered_df['charges_ratio'].fillna(0)).round(2)\n",
        "\n",
        "Engineered_df['contract_churn_interaction'] = df_encoded['Contract'].astype(str) + \"_\" + df_encoded['Churn'].astype(str)#I donit Know if this good\n",
        "\n",
        "Engineered_df['senior_internet'] = df_encoded.apply(lambda x: 'Yes' if x['SeniorCitizen'] == 1\n",
        "                                              and x['InternetService_No'] != 'Yes' else 'No', axis=1)\n",
        "Engineered_df[\"senior_internet\"].replace({\"Yes\": 1, \"No\": 0}, inplace=True)\n",
        "\n",
        "Engineered_df[\"Customer_score\"] = (df_encoded[\"tenure\"] * df_encoded[\"MonthlyCharges\"] * 0.5 + df_encoded[\"tenure\"] * df_encoded[\"TotalCharges\"] * 0.5)\n",
        "Engineered_df[\"Customer_score\"].round(2)\n",
        "Engineered_df[\"Tech_savvy\"] = (\n",
        "    (df_encoded[\"OnlineBackup\"] ) +\n",
        "    (df_encoded[\"DeviceProtection\"]) +\n",
        "    (df_encoded[\"TechSupport\"]).astype(int))\n",
        "Engineered_df[\"LoyaltyScore\"] = (Engineered_df[\"tenure\"] * Engineered_df[\"Contract\"] * Engineered_df[\"TotalCharges\"]).rank(pct=True).round(2)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate 75th percentiles\n",
        "monthly_charges_75th = df_encoded['MonthlyCharges'].quantile(0.75)\n",
        "total_charges_75th = df_encoded['TotalCharges'].quantile(0.75)\n",
        "\n",
        "\n",
        "Engineered_df['HighValueCustomer'] = (\n",
        "    (df_encoded['MonthlyCharges'] >= monthly_charges_75th) |\n",
        "    (df_encoded['TotalCharges'] >= total_charges_75th)\n",
        ").astype(int).rank(pct=True)\n",
        "\n",
        "\n",
        "#NEW\n",
        "Engineered_df['tenure_ratio'] = Engineered_df['tenure'] / Engineered_df['MonthlyCharges']\n",
        "Engineered_df['customer_loyalty_interaction'] = Engineered_df['Customer_score'] * Engineered_df['LoyaltyScore']\n",
        "Engineered_df['tech_loyalty_interaction'] = Engineered_df['Tech_savvy'] * Engineered_df['LoyaltyScore']\n",
        "Engineered_df['high_value_loyalty_interaction'] = Engineered_df['HighValueCustomer'] * Engineered_df['LoyaltyScore']\n",
        "Engineered_df['tech_high_value_interaction'] = Engineered_df['Tech_savvy'] * Engineered_df['HighValueCustomer']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "len(Engineered_df.columns.to_list())"
      ],
      "metadata": {
        "id": "AyLUBprkIrH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Engineered_df.head(100)"
      ],
      "metadata": {
        "id": "jhcDnxdPYjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Engineered_df[\"Tech_savvy\"].value_counts()"
      ],
      "metadata": {
        "id": "USwh2O51wFcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=Engineered_df, x='senior_internet', hue='Churn')\n",
        "\n",
        "\n",
        "plt.title('Distribution of Senior Citizens with Internet Service vs Churn')\n",
        "plt.xlabel('Senior Citizen with Internet Service')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "16lpEXjP8-PJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.hist(Engineered_df[\"Churn\"] , bins = 60 , edgecolor = \"black\")\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel(\"Contract\")\n",
        "plt.title(f\"Churn and Contract\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OtERTClk4oGF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the Mean for num_services : \",Engineered_df['num_services'].mean().round(2))\n",
        "print(\"the Median for num_services : \",Engineered_df['num_services'].median())\n",
        "print(\"the Range for num_services : \",Engineered_df['num_services'].max()-Engineered_df['num_services'].min())\n",
        "\n",
        "print(\"\\nthe Mean for charges_ratio : \",Engineered_df['charges_ratio'].mean().round(2))\n",
        "print(\"the Median for charges_ratio : \",Engineered_df['charges_ratio'].median())\n",
        "print(\"the Range for charges_ratio : \",Engineered_df['charges_ratio'].max()-Engineered_df['charges_ratio'].min())"
      ],
      "metadata": {
        "id": "BW1M4wldqHTp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4WNUNWbtACH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Distributions and Churn Analysis', fontsize=16)\n",
        "\n",
        "sns.histplot(Engineered_df, x='MonthlyCharges', kde=True, hue='Churn', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Monthly Charges Distribution by Churn')\n",
        "axes[0, 0].set_xlabel('Monthly Charges')\n",
        "\n",
        "sns.histplot(Engineered_df, x='TotalCharges', kde=True, hue='Churn', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Total Charges Distribution by Churn')\n",
        "axes[0, 1].set_xlabel('Total Charges')\n",
        "\n",
        "sns.histplot(Engineered_df, x='num_services', kde=False, hue='Churn', multiple=\"dodge\", ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Number of Services Distribution by Churn')\n",
        "axes[1, 0].set_xlabel('Number of Services')\n",
        "\n",
        "sns.boxplot(Engineered_df, x='Churn', y='charges_ratio', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Charges Ratio by Churn')\n",
        "axes[1, 1].set_xlabel('Churn')\n",
        "axes[1, 1].set_ylabel('Charges Ratio')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e-Fe5U6Er8na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Monthly Charges Distribution by Churn (Top Left)**\n",
        "Customers with higher monthly charges tend to churn more frequently than those with lower charges.\n",
        "Offering plans or promotions targeting high-spending customers could reduce churn, as they may feel dissatisfied with the value they receive.\n",
        "\n",
        "**2. Total Charges Distribution by Churn (Top Right)**\n",
        "Customers who leave tend to have lower total charges overall, possibly indicating that churned customers are either newer customers or less engaged over time.\n",
        "A focus on engagement strategies early in the customer lifecycle might help retain these customers.\n",
        "\n",
        "**3. Number of Services Distribution by Churn (Bottom Left)**\n",
        "Customers using fewer services are more likely to churn. This suggests that bundling services together could increase retention.\n",
        "Encouraging customers to adopt multiple services may create stickiness and reduce churn rates.\n",
        "\n",
        "**4. Charges Ratio by Churn (Bottom Right)**\n",
        "A higher charges ratio may correlate with longer-term or more engaged customers. Customers with lower ratios (possibly indicating limited usage or engagement) are more likely to churn.\n",
        "Offering targeted engagement strategies to customers with lower usage ratios could help reduce churn.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NqZOOkb45-Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# إعداد المخططات\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Customer_score Boxplot\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.boxplot(x='Churn', y='Customer_score', data=Engineered_df)\n",
        "plt.title('Customer Score vs Churn')\n",
        "\n",
        "# Tech_savvy Boxplot\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.boxplot(x='Churn', y='Tech_savvy', data=Engineered_df)\n",
        "plt.title('Tech Savvy vs Churn')\n",
        "\n",
        "# LoyaltyScore Boxplot\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.boxplot(x='Churn', y='LoyaltyScore', data=Engineered_df)\n",
        "plt.title('Loyalty Score vs Churn')\n",
        "\n",
        "# HighValueCustomer Boxplot\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.boxplot(x='Churn', y='HighValueCustomer', data=Engineered_df)\n",
        "plt.title('High Value Customer vs Churn')\n",
        "\n",
        "# عرض المخططات\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ehi2Caf61jcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Customer Score vs Churn (Top Left):**\n",
        "A higher customer score seems to correlate with customers not churning.\n",
        "However, the presence of outliers suggests that even high-scoring customers can churn, which could indicate external factors (e.g., service issues or competition) influencing their behavior.\n",
        "\n",
        "**2. Tech Savvy vs Churn (Top Right)**\n",
        "Tech-savviness seems to be a factor in retention. Customers who are more tech-savvy (comfortable with digital services) are less likely to churn.\n",
        "Less tech-savvy customers may struggle with or avoid engaging with services, leading to a higher likelihood of churn.\n",
        "\n",
        "**3. Loyalty Score vs Churn (Bottom Left)**\n",
        "Loyalty scores have a significant impact on churn. A higher loyalty score suggests greater engagement and satisfaction, which helps retain customers.\n",
        "The clear gap in the distributions between churned and non-churned groups indicates that loyalty is a critical factor in customer retention.\n",
        "\n",
        "**4. High Value Customer vs Churn (Bottom Right)**\n",
        "Being a high-value customer does not seem to influence churn significantly. This could mean that even high-value customers may churn if other issues (e.g., poor service or lack of engagement) are present.\n",
        "This finding emphasizes that retention efforts should focus on other factors like loyalty and customer experience, not just monetary value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jTyYgBmP3yyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = Engineered_df.corr()\n",
        "\n",
        "# Strong Correlation Filtered Heatmap (only shows |r| > 0.5)\n",
        "strong_corr_matrix = correlation_matrix.copy()\n",
        "strong_corr_matrix[np.abs(strong_corr_matrix) < 0.5] = 0  # Mask weak correlations\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(strong_corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title('Strong Correlations (|r| > 0.5)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZbIDdtnbuCoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = Engineered_df.corr()\n",
        "features_of_interest = ['MonthlyCharges', 'InternetService_Fiber optic','TotalCharges','InternetService_No' ,'Tech_savvy', 'LoyaltyScore',\n",
        "                        'HighValueCustomer','tenure','StreamingTV']\n",
        "\n",
        "selected_corr_matrix = correlation_matrix.loc[features_of_interest, features_of_interest]\n",
        "\n",
        "selected_corr_matrix[np.abs(selected_corr_matrix) < 0.5] = 0  # Mask weak correlations\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(selected_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix for Selected Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P14wdqMP7Gnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# إعداد مخطط الانتشار مع ألوان مختلفة واضحة\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=Engineered_df,\n",
        "    x='TotalCharges',\n",
        "    y='LoyaltyScore',\n",
        "    hue='Churn',\n",
        "    palette={0: 'blue', 1: 'red'},  # تعيين الأزرق للعملاء غير المغادرين والأحمر للعملاء المغادرين\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "plt.title('Relationship between Total Charges and Loyalty Score by Churn')\n",
        "plt.xlabel('Total Charges')\n",
        "plt.ylabel('Loyalty Score')\n",
        "plt.legend(title='Churn', labels=['No Churn', 'Churn'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7uVvpI42DVxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot suggests that a higher loyalty score and total charges are generally indicative of customer retention, while lower loyalty scores are more common among customers who churn. Further analysis could explore these clusters or segments to understand what drives loyalty and retention."
      ],
      "metadata": {
        "id": "qEJCuiqlEYmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=Engineered_df,\n",
        "    x='TotalCharges',\n",
        "    y='tenure',\n",
        "    hue='Churn',\n",
        "    palette={0: 'blue', 1: 'red'},\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "plt.title('Relationship between Total Charges and tenure by Churn')\n",
        "plt.xlabel('Total Charges')\n",
        "plt.ylabel('tenure')\n",
        "plt.legend(title='Churn', labels=['No Churn', 'Churn'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T8UiJ8eiEZSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Churn (Red)**: Customers who churn tend to have shorter tenures, mostly concentrated within the lower tenure range (0–20 months). This suggests that new or short-term customers are more likely to leave.\n",
        "\n",
        "**No Churn (Blue)**: Customers who do not churn span across the entire tenure range, with a heavy concentration towards higher tenures (20–70 months). This implies that retention improves as customers stay longer with the company.\n",
        "\n",
        "In summary, this plot suggests that tenure is a strong indicator of customer loyalty, and most churn happens early in the customer lifecycle. Efforts to increase retention may be most effective when directed at customers with low tenure.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6rL6kotuFh_h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4R7ylhalFhjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Engineered_df.drop(columns=['customerID'] , inplace=True)\n",
        "#df_encoded.drop(columns=['customerID'] , inplace=True)"
      ],
      "metadata": {
        "id": "7MublOV091ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming df_encoded is your DataFrame\n",
        "X = Engineered_df.drop(columns=['Churn'])\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Specify columns to scale\n",
        "columns_to_scale = ['MonthlyCharges', 'TotalCharges', 'tenure', 'charges_ratio']  # Specify the correct column names\n",
        "\n",
        "# Create a scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform only the specified columns in the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "\n",
        "# Transform only the specified columns in the test set\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use GridSearchCV to search for the best parameters\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Retrieve the best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validated Accuracy:\", grid_search.best_score_)\n",
        "print(\"Accuracy Score on Test Data:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ZYoX3lNgnWNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming Engineered_df is your DataFrame\n",
        "X = Engineered_df.drop(columns=['Churn'])\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify columns to scale\n",
        "columns_to_scale = ['MonthlyCharges', \"TotalCharges\" , \"Customer_score\", \"charges_ratio\"]# , \"customer_usefulness\"]\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler2 = StandardScaler()\n",
        "\n",
        "# Fit and transform only the specified columns in the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "\n",
        "\n",
        "# Transform only the specified columns in the validation and test sets\n",
        "X_val_scaled = X_val.copy()\n",
        "X_val_scaled[columns_to_scale] = scaler.transform(X_val[columns_to_scale])\n",
        "\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "\n",
        "# Print shapes\n",
        "print(\"X_train shape:\", X_train_scaled.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val_scaled.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = model.predict(X_val_scaled)\n",
        "\n",
        "# Print validation results\n",
        "print(\"Validation Accuracy Score:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
        "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Print test results\n",
        "print(\"Test Accuracy Score:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "id": "lCMLSzZu9Yiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW\n",
        "\n",
        "features = ['Customer_score', 'Tech_savvy', 'LoyaltyScore', 'HighValueCustomer']\n",
        "X = Engineered_df[features]\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "#model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "k4aur9CHImd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# اختيار الميزات الجديدة والقديمة\n",
        "features = [\n",
        "    'Customer_score',\n",
        "    'Tech_savvy',\n",
        "    'LoyaltyScore',\n",
        "    'HighValueCustomer',\n",
        "    'tenure_ratio',\n",
        "    'customer_loyalty_interaction',\n",
        "    'tech_loyalty_interaction',\n",
        "    'high_value_loyalty_interaction',\n",
        "    'tech_high_value_interaction'\n",
        "]\n",
        "\n",
        "# إعداد X و y\n",
        "X = Engineered_df[features]\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# تقسيم البيانات إلى مجموعة تدريب واختبار\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# إنشاء وتدريب نموذج Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# التنبؤ واختبار النموذج\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# تقييم النموذج\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "PZKHm-eEKksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "features = [\n",
        "    'Customer_score',\n",
        "    'Tech_savvy',\n",
        "    'LoyaltyScore',\n",
        "    'HighValueCustomer',\n",
        "    'tenure_ratio',\n",
        "    'customer_loyalty_interaction',\n",
        "    'tech_loyalty_interaction',\n",
        "    'high_value_loyalty_interaction',\n",
        "    'tech_high_value_interaction'\n",
        "]\n",
        "\n",
        "X = Engineered_df[features]\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=10, callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "qZmEDUMhLvql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming Engineered_df is your DataFrame\n",
        "X = Engineered_df.drop(columns=['Churn'])\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify columns to scale\n",
        "columns_to_scale = ['MonthlyCharges', \"TotalCharges\", \"Customer_score\", \"charges_ratio\"]\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform only the specified columns in the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "\n",
        "# Transform only the specified columns in the validation and test sets\n",
        "X_val_scaled = X_val.copy()\n",
        "X_val_scaled[columns_to_scale] = scaler.transform(X_val[columns_to_scale])\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "# Print shapes\n",
        "print(\"X_train shape:\", X_train_scaled.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val_scaled.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "# Set up hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],  # Optimization algorithms\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None]  # Regularization techniques\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set using the best model\n",
        "y_val_pred = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Print validation results\n",
        "print(\"Validation Accuracy Score:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
        "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Print test results\n",
        "print(\"Test Accuracy Score:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "6-b43M3uXXok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming Engineered_df is your DataFrame\n",
        "X = Engineered_df.drop(columns=['Churn'])\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify columns to scale\n",
        "columns_to_scale = ['MonthlyCharges', \"TotalCharges\", \"Customer_score\", \"charges_ratio\"]\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform only the specified columns in the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "\n",
        "# Transform only the specified columns in the validation and test sets\n",
        "X_val_scaled = X_val.copy()\n",
        "X_val_scaled[columns_to_scale] = scaler.transform(X_val[columns_to_scale])\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "# Print shapes\n",
        "print(\"X_train shape:\", X_train_scaled.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val_scaled.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "# Set up hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
        "    'l1_ratio': [0.5]  # Example value; adjust as needed\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on training set\n",
        "y_train_pred = best_model.predict(X_train_scaled)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Print validation results\n",
        "print(\"Validation Accuracy Score:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
        "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Print test results\n",
        "print(\"Test Accuracy Score:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Check for overfitting\n",
        "print(\"\\nTraining Accuracy Score:\", train_accuracy)\n",
        "if train_accuracy > accuracy_score(y_val, y_val_pred):\n",
        "    print(\"Potential Overfitting Detected: Training accuracy is greater than validation accuracy.\")\n",
        "else:\n",
        "    print(\"No Overfitting Detected: Training accuracy is not greater than validation accuracy.\")\n"
      ],
      "metadata": {
        "id": "WHJ06kBwkTbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming Engineered_df is your DataFrame\n",
        "X = Engineered_df.drop(columns=['Churn'])\n",
        "y = Engineered_df['Churn']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify columns to scale\n",
        "columns_to_scale = ['MonthlyCharges', 'TotalCharges', 'Customer_score', 'charges_ratio']\n",
        "\n",
        "# Create a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform only the specified columns in the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "\n",
        "# Introduce Gaussian noise to the training features\n",
        "noise_std = 150  # Increased noise standard deviation\n",
        "for column in columns_to_scale:\n",
        "    noise = np.random.normal(0, noise_std, size=X_train_scaled[column].shape)\n",
        "    X_train_scaled[column] += noise\n",
        "\n",
        "# Introduce random label noise\n",
        "label_noise_ratio = 0.3  # 10% of the labels will be randomly flipped\n",
        "n_noisy_labels = int(label_noise_ratio * y_train.shape[0])\n",
        "random_indices = np.random.choice(y_train.index, n_noisy_labels, replace=False)\n",
        "y_train_noisy = y_train.copy()\n",
        "y_train_noisy[random_indices] = np.random.choice([0, 1], n_noisy_labels)  # Flip the labels\n",
        "\n",
        "# Transform only the specified columns in the validation and test sets\n",
        "#X_val_scaled = X_val.copy()\n",
        "#X_val_scaled[columns_to_scale] = scaler.transform(X_val[columns_to_scale])\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "# Print shapes\n",
        "print(\"X_train shape:\", X_train_scaled.shape)\n",
        "print(\"y_train shape:\", y_train_noisy.shape)\n",
        "#print(\"X_val shape:\", X_val_scaled.shape)\n",
        "#print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "# Set up hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None]\n",
        "}\n",
        "\n",
        "# Create Grid Search\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=300), param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train_noisy)\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores = cross_val_score(best_model, X_train_scaled, y_train_noisy, cv=5)\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(cv_scores))\n",
        "\n",
        "# Make predictions on the validation set using the best model\n",
        "#y_val_pred = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Print validation results\n",
        "#print(\"Validation Accuracy Score:\", accuracy_score(y_val, y_val_pred))\n",
        "#print(\"Validation Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
        "#print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Print test results\n",
        "print(\"Test Accuracy Score:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "RHsg1TIqQhZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K006dpByQ576"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXWq9kr8Q6HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Encode target variable as binary\n",
        "df_encoded['Churn'] = df_encoded['Churn'].replace({2: 0, 1: 1})\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data for RNN input (samples, timesteps, features)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "# Initialize the RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# First LSTM Layer\n",
        "model.add(LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True, kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Second LSTM Layer\n",
        "model.add(LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001)))  # return_sequences=False for the last LSTM\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense Layers\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Predictions\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "4BMx83HWAf39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "pB0me2YI96Ca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}